# Lab 5: Ingest and Analyse real-time data with Event Hubs and Stream Analytics
In this lab you will use an Azure Logic App to simulate high-frequency stock market of the NYSE. Every second the LogicApp generates a random number of stock purchase transactions with variable amounts for 5 of the biggest tech companies in the world. The LogicApp then sends stock transaction messages to Event Hubs. You will use Stream Analytics queries to join hot and cold data streams to process the high volume of transactions and generate aggregate calculations. The results will be sent to a real-time dataset in Power BI.

**IMPORTANT**: This lab requires you have a valid Power BI account. If you don’t have one you can register for a 60-day trial here: https://powerbi.microsoft.com/en-us/power-bi-pro/

The estimated time to complete this lab is: **60 minutes**.

## Microsoft Learn & Technical Documentation

The following Azure services will be used in this lab. If you need further training resources or access to technical documentation please find in the table below links to Microsoft Learn and to each service's Technical Documentation.

Azure Service | Microsoft Learn | Technical Documentation|
--------------|-----------------|------------------------|
Azure Data Lake Gen2 | [Large Scale Data Processing with Azure Data Lake Storage Gen2](https://docs.microsoft.com/en-us/learn/paths/data-processing-with-azure-adls/) | [Azure Data Lake Gen2 Technical Documentation](https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction)
Azure Logic Apps | [Build automated workflows to integrate data and apps with Azure Logic Apps](https://docs.microsoft.com/en-us/learn/paths/build-workflows-with-logic-apps/) | [Azure Logic Apps Technical Documentation](https://docs.microsoft.com/en-us/azure/logic-apps/)
Azure Event Hubs | [Enable reliable messaging for Big Data applications using Azure Event Hubs](https://docs.microsoft.com/en-us/learn/modules/enable-reliable-messaging-for-big-data-apps-using-event-hubs/) | [Azure Event Hubs Technical Documentation](https://docs.microsoft.com/en-us/azure/event-hubs/)
Azure Stream Analytics | [Implement a Data Streaming Solution with Azure Streaming Analytics](https://docs.microsoft.com/en-us/learn/paths/implement-data-streaming-with-asa/) | [Azure Stream Analytics Technical Documentation](https://docs.microsoft.com/en-us/azure/stream-analytics/)
Power BI | [Create and use analytics reports with Power BI](https://docs.microsoft.com/en-us/learn/paths/create-use-analytics-reports-power-bi/) | [Power BI Technical Documentation](https://docs.microsoft.com/en-us/power-bi/)

## Lab Architecture
![Lab Architecture](./Media/Lab5-Image01.png)

Step     | Description
-------- | -----
![](./Media/Orange1.png) | Review the Azure Logic App logic that simmulates the NYSE transaction stream sent to EventHubs
![](./Media/Orange2.png) | Save simmulated NYSE stock transaction messages into your data lake for future analysis (cold path)
![](./Media/Orange3.png) | Send stream of NYSE stock transaction messages to Stream Analytics for real-time analytics (hot path)
![](./Media/Orange4.png) | Incorporate Stock Company reference data into your stream processing logic
![](./Media/Orange5.png) | Visualize real-time data generated by Stream Analytics with Power BI

**IMPORTANT**: Some of the Azure services provisioned require globally unique name and a “-suffix” has been added to their names to ensure this uniqueness. Please take note of the suffix generated as you will need it for the following resources in this lab:

Name	                     |Type
-----------------------------|--------------------
synapsedatalake*suffix*	         |Storage Account
ADPEventHubs-*suffix*	     |Event Hubs Namespace
SynapseStreamAnalytics-*suffix*	 |Stream Analytics job

## Review the Azure LogicApp implementation

In this section you will review the implementation of the LogicApp used to simmulate high-frequency stock purchase transactions. These transactions will be formatted as JSON messages and sent to Event Hubs for processing. All steps required to generate the stock transaction messages have alreay been done for you and no further changes are required in this section.

![](./Media/Lab5-Image10.png)

**IMPORTANT**|
-------------|
**Execute these steps on your host computer**|

1.	In the Azure Portal, go to the lab resource group and locate the Logic App resource **ADPLogicApp**.

2.	On the **ADPLogicApp** menu, click **Logic app designer** to open the design panel.

    ![](./Media/Lab5-Image11.png)

3.	On the **Logic app designer** panel, note that the **Recurrence** trigger is set to execute every 1 second:

    ![](./Media/Lab5-Image12.png)

4.	The next two steps **Initialize Config Settings** and **Parse Config Settings** define the parameters used to generate the stock purchase messages.

    ![](./Media/Lab5-Image13.png)

5.	The **Initialize messageCount** step is used to initialize a variable used to count the number of messages generated. It's initial value is set to 1.

    ![](./Media/Lab5-Image14.jpg)

6.	In the **Until numberOfMessages is achieved** loop a piece of JavaScript is executed to generate the stock purchase transaction message. The message is then sent to the **nysestocktrade** Event Hub. The loop repeats its execution generating a random number of messages between 1 and 10 for every execution of the LogicApp.

    ![](./Media/Lab5-Image15.png)

7. The format of each stock purchase transaction message generated looks like this:

```json
{
  "StockTicker": "MSFT",
  "Quantity": 120,
  "Price": 83.63,
  "TradeTimestamp": "2019-11-24T00:21:50.207Z"
}
```

8. Return to the **Overview** panel. Note that the LogicApp is disabled by default. Click **Enable** to enable the LogicApp to start firing every second.

    ![](./Media/Lab5-Image61.png)


## Configure Capture for Event Hubs
In this section you will prepare Event Hubs to ingest NYSE stock trade messages generated by the LogicApp and save to your Synapse Data Lake account.

![](./Media/Lab5-Image06.png)

**IMPORTANT**|
-------------|
**Execute these steps on your host computer**|

1.	In the Azure Portal, go to the lab resource group and locate the Event Hubs resource **ADPEventHubs-*suffix***. 

2.	On the **Event Hubs** panel, note that the **nysestocktrade** Event Hub has already been created for you. This is the same Event Hub you saw referenced by the LogicApp in the previous section. 

    ![](./Media/Lab5-Image07.png)

3. Click on the **nysestocktrade** Event Hub to open its settings. Then click on the **Capture** item on the left-hand side menu.

    ![](./Media/Lab5-Image55.png)

4.	Enter the following details:
    <br>- **Capture**: On
    <br>- **Time window (minutes)**: 1
    <br>- **Do not emit empty files when no events occur during the capture time window**: Checked.
    <br>- **Capture Provider**: Azure Storage
    <br>- **Azure Storage Container**: [select the **nysestocktrade** container in your **synapsedatalake*suffix*** storage account]
7.	Leave remaining fields with their default values.
8.	Click **Save Changes**.

    ![](./Media/Lab5-Image09.png)

### Optional: Investigate NYCStockTrades Container contents

1. On your SynapseDataLake account, navigate to the NYCStockTrades container you created in the previous section.

2. You should be able to see the folder structure created by Event Hubs Capture with AVRO files containing the individual stock purchase transaction messages generated by the LogicApp. These files can then be used in other analytics worlkloads whenever the granular detail about each individual transaction is required.

    ![](./Media/Lab5-Image62.png)


## Configure Stream Analytics Job
In this section you will configure your Stream Analytics job to join hot and cold data streams and execute queries on data sent by Event Hubs and generate outputs to Power BI.

![](./Media/Lab5-Image28.png)

**IMPORTANT**|
-------------|
**Execute these steps on your host computer**|

1.	In the Azure Portal, go to the lab resource group and locate the Stream Analytics resource **SynapseStreamAnalytics-*suffix***. 

2.	On the Inputs panel, click **+ Add stream input** button and select **Event Hub** to create a new input stream.

    ![](./Media/Lab5-Image29.png)

3.	On the **Event Hub New input** blade enter the following details:
    <br>- **Input alias**: NYSEStockTrades
    <br>- **Event Hub namespace**: ADPEventHubs-suffix
    <br>- **Event Hub name > Use existing**: nysestocktrade
    <br>- **Event Hub policy name > Use existing**: RootManageSharedAccessKey
    <br>- **Event Hub consumer group > Use existing**: $Default

4.	Leave remaining fields with their default values.

    ![](./Media/Lab5-Image30.png)

5. Now click **+ Add reference input** button and select **SQL Database** to create a new reference data input stream.

    ![](./Media/Lab5-Image56.png)

6.	On the **SQL Database New input** blade enter the following details:
    <br>- **Input alias**: NYSEStockCompanies
    <br>- **Storage account for this job**: [select your synapsedatalake*suffix* storage account]
    <br>- **Select SQL Database from your subscriptions**: Checked
    <br>- **Subscription**: your Azure subscription
    <br>- **Database**: NYCDataSets
    <br>- **Server name**: operationalsql-*suffix*.database.windows.net
    <br>- **User name**: adpadmin
    <br>- **Password**: P@ssw0rd123!
    <br>- **Refresh periodically**: Off
    <br>- **Snapshot query**:
    ```sql
    select [StockTicker]
      ,[CompanyName]
    from [NYC].[NYSE_StockTickerLookup]
    ```
    This reference table contains static data about the companies:
    
    ![](./Media/Lab5-Image59.png)

    ![](./Media/Lab5-Image57.png)

7. Click **Save** to save your reference input stream and return to the Inputs panel.

8.	Click on the **Outputs** panel on the left-hand side menu. Once it is loaded, click **+ Add** button and select **Power BI** to create a new output stream.

    ![](./Media/Lab5-Image31.png)


9.	On the **Power BI New Output** blade, click **Authorize** to authenticate with Power BI. Enter your credentials to authenticate.
    
    ![](./Media/Lab5-Image58.png)

10.	Once authenticated, enter the following details:
    <br>- **Output alias**: StockTradeByCompany
    <br>- **Authentication Mode**: User Token
    <br>- **Group Workspace**: My Workspace
    <br>- **Dataset name**: StockTradeByCompany
    <br>- **Table name**: StockTradeByCompany

**IMPORTANT**: Set **Authentication Mode** to **User Token** before you can select **My Workspace** for **Group Workspace**.

![](./Media/Lab5-Image32.png)

11.	Leave remaining fields with their default values.

12.	Click **Save** to save your output stream and return to the **Outputs** panel.

13.	Repeat the process to create another Power BI Output. This time enter the following details:
    <br>- **Output alias**: StockTradeTotals
    <br>- **Authentication Mode**: User Token
    <br>- **Group Workspace**: My Workspace
    <br>- **Dataset name**: StockTradeTotals
    <br>- **Table name**: StockTradeTotals

14.	Click **Save** to save your output stream and return to the **Outputs** panel.

    ![](./Media/Lab5-Image33.png)

15.	On the **Query** panel, note the inputs and outputs you created in the previous steps. 

    ![](./Media/Lab5-Image34.png)

16.	Enter the following SQL commands in the query window.

```sql
--Total amount traded broken down by company in the last 30 seconds and calculated every 5 seconds
SELECT
    Company.CompanyName
    , sum(Trade.Quantity * Trade.Price) as TradedAmount
    , System.Timestamp as WindowDateTime
INTO
    [StockTradeByCompany]
FROM
    [NYSEStockTrades] as Trade TIMESTAMP BY TradeTimestamp
    INNER JOIN [NYSEStockCompanies] as Company
        on Trade.StockTicker = Company.StockTicker
GROUP BY Company.CompanyName, HoppingWindow(second, 30, 5)

--Total amount traded and total number of transactions in the last 30 seconds and calculated every 5 seconds
SELECT
    sum(Trade.Quantity * Trade.Price) as TotalTradedAmount
    , count(*) as TotalTradeCount
    , System.Timestamp as WindowDateTime
INTO
    [StockTradeTotals]
FROM
    [NYSEStockTrades] as Trade TIMESTAMP BY TradeTimestamp
GROUP BY HoppingWindow(second, 30, 5)
```

17.	Click **Save query**.

18.	On the **Overview** panel, click **Start** to start the Stream Analytics job.

    ![](./Media/Lab5-Image35.png)

18. On the **Start job** blade, select **Now** and click the **Start** button.

    ![](./Media/Lab5-Image60.png)


# Create Power BI Dashboard to Visualise Real-Time Data
In this section you will log on to the Power BI portal to create a dashboard to visualize real-time stock transactions statistics data sent by Stream Analytics.

![](./Media/Lab5-Image36.png)

**IMPORTANT**|
-------------|
**Execute these steps on your host computer**|

1.	Open a new browser tab and navigate to https://www.powerbi.com
2.	Enter your credentials to authenticate with the Power BI service.

    ![](./Media/Lab5-Image37.png)

3.	Once authenticated, open the **Workspaces** menu and click **My Workspace** at the top of the Workspaces list.

    ![](./Media/Lab5-Image38.png)

4.	Navigate to the **Datasets** tab and verify that two datasets have been created by Stream Analytics: **StockTradeByCompany** and **StockTradeTotals**.

    ![](./Media/Lab5-Image39.png)

5.	On the top right-hand side corner click **+ Create** and then click **Dashboard** from the dropdown menu to create a new dashboard.

    ![](./Media/Lab5-Image40.png)

6.	Type **NYSE Trade Activity** in the **Dashboard name** field and click **Create**.

7.	Click on the *(elipsis)* **...** button from the toolbar and then click on the **+ Add tile** menu item. 

![](./Media/Lab5-Image53.png)

8.	On the **Add tile** blade, select **Custom Streaming Data** under the **Real-Time Data** section.

9.	Click **Next**.

    ![](./Media/Lab5-Image41.png)

10.	On the **Add a custom streaming data tile** blade, select the **StockTradeTotals** dataset.

11.	Click **Next**.

    ![](./Media/Lab5-Image42.png)

12.	On the **Visualization Type** field select **Card**.

13.	On the **Fields** field select **TotalTradedAmount**.

    ![](./Media/Lab5-Image43.png)

14. Click on the brush icon to set the **Value decimal places** field to **2**.

    ![](./Media/Lab5-Image63.png)

15.	Click **Next**.

16.	On the **Tile details** blade, enter the following details:
    <br>- **Title**: Total Traded Amount
    <br>- **Subtitle**: in the last 30 seconds

    ![](./Media/Lab5-Image44.png)

17.	Leave remaining fields with their default values. Click **Apply**.

18.	Repeat the process to create another tile, this time to display the total trade count. Use the following details:
    <br>- **Dataset**: StockTradeTotals
    <br>- **Visualization Type**: Card
    <br>- **Fields**: TotalTradeCount
    <br>- **Details > Title**: Total Trade Count
    <br>- **Details > Subtitle**: in the last 30 seconds

    ![](./Media/Lab5-Image45.png)

19.	You should be able to see the values for both tiles changing every 5 seconds. 

20. Repeat the process to create another tile, this time to display the historical values for TotalTradeCount over the last 5 minutes.
    <br>- **Dataset**: StockTradeTotals
    <br>- **Visualization Type**: Line Chart
    <br>- **Axis**: WindowDateTime
    <br>- **Value**: TotalTradeCount
    <br>- **Time window to display**: 5 minutes
    <br>- **Details > Title**: Total Trade Count
    <br>- **Details > Subtitle**: 5 min history window

    ![](./Media/Lab5-Image64.png)


21. Repeat the process to create another tile, this time to display the total traded amount broken down by company.
    <br>- **Dataset**: StockTradeByCompany
    <br>- **Visualization Type**: Clustered bar chart
    <br>- **Axis**: CompanyName
    <br>- **Legend**: CompanyName
    <br>- **Value**: TradedAmount
    <br>- **Details > Title**: Traded Amount by Company
    <br>- **Details > Subtitle**: in the last 30 seconds

    ![](./Media/Lab5-Image65.png)

22. Your real-time dashboard should look similar to the picture below. Every tile should be refreshed approximately every 5 seconds.

    ![](./Media/Lab5-Image66.png)

